{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. Apache Spark binary (https://spark.apache.org/)\n",
    "2. For Windows: winutils (https://medium.com/@dvainrub/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3)\n",
    "3. Setting ```JAVA_HOME```, ```SPARK_HOME```, and ```HADOOP_HOME```\n",
    "4. Python 3.x (from Anaconda distribution)\n",
    "5. ```findspark``` https://pypi.org/project/findspark/\n",
    "6. Jupyter Notebook (available from Anaconda installation)\n",
    "\n",
    "### References\n",
    "https://spark.apache.org/docs/2.3.3/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark to read SPARK_HOME and HADOOP_HOME\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x10b22bba8>\n"
     ]
    }
   ],
   "source": [
    "# Print Spark object ID\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Users/gunstringer/Downloads/epa_hap_daily_summary.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------+---+---------+-----------+-------+--------------------+---------------+------------------+-------------------+--------------------+----------+-----------------+-------------------+---------------+---------------+--------------+----+-----------+--------------------+--------------------+--------------------+------------+----------------+-------------+--------------------+-------------------+\n",
      "|state_code|county_code|site_num|parameter_code|poc| latitude|  longitude|  datum|      parameter_name|sample_duration|pollutant_standard|         date_local|    units_of_measure|event_type|observation_count|observation_percent|arithmetic_mean|first_max_value|first_max_hour| aqi|method_code|         method_name|     local_site_name|             address|  state_name|     county_name|    city_name|           cbsa_name|date_of_last_change|\n",
      "+----------+-----------+--------+--------------+---+---------+-----------+-------+--------------------+---------------+------------------+-------------------+--------------------+----------+-----------------+-------------------+---------------+---------------+--------------+----+-----------+--------------------+--------------------+--------------------+------------+----------------+-------------+--------------------+-------------------+\n",
      "|        42|        125|    5001|         88103|  5|40.445278| -80.420833|  WGS84|    Arsenic PM2.5 LC|        24 HOUR|              null|2005-12-30 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|          0.003|          0.003|             0|null|        811|Met OneSASS Teflo...|                null|HILLMAN STATE PAR...|Pennsylvania|      Washington|Not in a city|       Pittsburgh PA|2015-07-22 00:00:00|\n",
      "|        48|        439|    1002|         43843| 15|32.805818| -97.356568|  WGS84|  Ethylene dibromide|        24 HOUR|              null|2013-09-19 00:00:00|Parts per billion...|      None|                1|              100.0|            0.0|            0.0|             0|null|        175|Passivated Canist...|Fort Worth Northwest|       3317 Ross Ave|       Texas|         Tarrant|   Fort Worth|Dallas-Fort Worth...|2014-03-25 00:00:00|\n",
      "|        22|        127|    9000|         88128|  1|32.057581| -92.435157|  WGS84|       Lead PM2.5 LC|        24 HOUR|              null|2001-11-12 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|        0.00228|        0.00228|             0|null|        802|IMPROVE Module A ...|                null|               Sikes|   Louisiana|            Winn|Not in a city|                null|2015-07-22 00:00:00|\n",
      "|        18|         89|      22|         45201|  1| 41.60668| -87.304729|  WGS84|             Benzene|         1 HOUR|              null|2016-05-23 00:00:00|Parts per billion...|      None|               23|               96.0|       0.413913|            1.4|             2|null|        128|PRECONCENTRATION ...|Gary-IITRI/ 1219....|201 MISSISSIPPI S...|     Indiana|            Lake|         Gary|Chicago-Napervill...|2017-02-20 00:00:00|\n",
      "|         6|         89|    3003|         88136|  1| 40.53999| -121.57646|  NAD83|     Nickel PM2.5 LC|        24 HOUR|              null|2001-08-02 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|         1.3E-4|         1.3E-4|             0|null|        802|IMPROVE Module A ...|Lassen Volcanic N...|MANZANITA LAKE RS...|  California|          Shasta|Not in a city|          Redding CA|2015-07-22 00:00:00|\n",
      "|        26|        163|      33|         82128|  1|42.306674| -83.148754|  WGS84|       Lead PM10 STP|        24 HOUR|              null|2008-10-03 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|         0.0163|         0.0163|             0|null|        109|Hi Vol SA/GMW 321...|PROPERTY OWNED BY...|        2842 WYOMING|    Michigan|           Wayne|     Dearborn|Detroit-Warren-De...|2015-07-22 00:00:00|\n",
      "|         6|         83|    9000|         88128|  1|34.733889|-120.008349|  WGS84|       Lead PM2.5 LC|        24 HOUR|              null|2015-08-04 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|            0.0|            0.0|             0|null|        800|IMPROVE Module A ...|San Rafael Wilder...|          San Rafael|  California|   Santa Barbara|Not in a city|Santa Maria-Santa...|2016-08-30 00:00:00|\n",
      "|        72|         61|       1|         88136|  5|18.425652| -66.115846|  WGS84|     Nickel PM2.5 LC|        24 HOUR|              null|2002-10-29 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|         0.0127|         0.0127|             0|null|        811|Met One SASS Tefl...|                null|USGS AND WATER RE...| Puerto Rico|        Guaynabo|     Guaynabo|San Juan-Carolina...|2015-07-22 00:00:00|\n",
      "|        29|        510|      85|         43505|  6|38.656498| -90.198646|  NAD83|Acrolein - Unveri...|        24 HOUR|              null|2006-02-10 00:00:00|Parts per billion...|      None|                1|              100.0|            0.0|            0.0|             0|null|        101|CANISTER SUBAMBIE...|        Blair Street|BLAIR STREET: 324...|    Missouri|  St. Louis City|    St. Louis|     St. Louis MO-IL|2015-07-22 00:00:00|\n",
      "|         6|         77|    1002|         12112|  8|37.950741|-121.268523|  NAD83|  Chromium (TSP) STP|        24 HOUR|              null|1992-10-03 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|          0.003|          0.003|             0|null|        304|LO-VOL-XONTECH 92...|   Stockton-Hazelton|HAZELTON-HD STOCKTON|  California|     San Joaquin|     Stockton|    Stockton-Lodi CA|2013-06-11 00:00:00|\n",
      "|        24|        510|       6|         43831|  1|39.340556| -76.582222|  WGS84|cis-13-Dichloropr...|        24 HOUR|              null|2008-10-27 00:00:00|Parts per billion...|      None|                1|              100.0|            0.0|            0.0|             0|null|        150|SS 6L- PRESSURIZE...|    Northeast Police|Northeast  Police...|    Maryland|Baltimore (City)|    Baltimore|Baltimore-Columbi...|2013-06-11 00:00:00|\n",
      "|        49|         17|     101|         88132|  1|37.618383|-112.174368|  WGS84|  Manganese PM2.5 LC|        24 HOUR|              null|2010-05-11 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|          0.001|          0.001|             0|null|        800|IMPROVE Module A ...|                null|     Bryce Canyon NP|        Utah|        Garfield|Not in a city|                null|2015-07-28 00:00:00|\n",
      "|        16|         23|     101|         88136|  1|43.460556|-113.562222|  WGS84|     Nickel PM2.5 LC|        24 HOUR|              null|2014-12-07 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|            0.0|            0.0|             0|null|        800|IMPROVE Module A ...|Craters of the Mo...|CRATERS OF THE MO...|       Idaho|           Butte|Not in a city|      Idaho Falls ID|2015-07-31 00:00:00|\n",
      "|        26|         77|     905|         82128|  1|42.278056| -85.541944|UNKNOWN|       Lead PM10 STP|        24 HOUR|              null|1994-04-08 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|           0.01|           0.01|             0|null|        103|HI-VOL-WEDDING-IN...|                null|FAIRGROUNDS 2500 ...|    Michigan|       Kalamazoo|    Kalamazoo|Kalamazoo-Portage MI|2013-06-11 00:00:00|\n",
      "|        48|        141|      44|         43824|  3|31.765685|-106.455227|  NAD83|   Trichloroethylene|        24 HOUR|              null|2006-05-05 00:00:00|Parts per billion...|      None|                1|              100.0|           0.01|           0.01|             0|null|        175|Passivated Canist...|    El Paso Chamizal|800 S San Marcial...|       Texas|         El Paso|      El Paso|          El Paso TX|2013-06-11 00:00:00|\n",
      "|        48|        183|       1|         43817|  1|32.378682| -94.711811|  NAD83| Tetrachloroethylene|        24 HOUR|              null|2000-04-12 00:00:00|Parts per billion...|      None|                1|              100.0|            0.0|            0.0|             0|null|        149|6L SUBATM CANISTE...|            Longview|Gregg Co Airport ...|       Texas|           Gregg|     Longview|         Longview TX|2013-06-11 00:00:00|\n",
      "|        48|        141|      44|         43218|  1|31.765685|-106.455227|  NAD83|        13-Butadiene|         1 HOUR|              null|2002-07-16 00:00:00|Parts per billion...|      None|               22|               92.0|       0.595455|            1.6|             5|null|        128|PRECONCENTRATION ...|    El Paso Chamizal|800 S San Marcial...|       Texas|         El Paso|      El Paso|          El Paso TX|2013-06-11 00:00:00|\n",
      "|        32|         33|     101|         88112|  1| 39.00512| -114.21593|  NAD83|   Chromium PM2.5 LC|        24 HOUR|              null|2001-11-09 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|            0.0|            0.0|             0|null|        802|IMPROVE Module A ...|Great Basin Natio...|GREAT BASIN NATIO...|      Nevada|      White Pine|Not in a city|                null|2015-07-22 00:00:00|\n",
      "|        27|        163|     436|         43803|  2| 44.84737|   -92.9954|  WGS84|          Chloroform|        24 HOUR|              null|2011-03-28 00:00:00|Parts per billion...|      None|                1|              100.0|           0.01|           0.01|             0|null|        113|SS-CANISTER-PRESS...|             MPC 436|    649 FIFTH STREET|   Minnesota|      Washington|St. Paul Park|Minneapolis-St. P...|2013-06-10 00:00:00|\n",
      "|        34|         39|       4|         88132|  5| 40.64144| -74.208365|  WGS84|  Manganese PM2.5 LC|        24 HOUR|              null|2002-03-24 00:00:00|Micrograms/cubic ...|      None|                1|              100.0|         0.0043|         0.0043|             0|null|        811|Met One SASS Tefl...|       Elizabeth Lab|Interchange 13 Ne...|  New Jersey|           Union|    Elizabeth|New York-Newark-J...|2015-07-22 00:00:00|\n",
      "+----------+-----------+--------+--------------+---+---------+-----------+-------+--------------------+---------------+------------------+-------------------+--------------------+----------+-----------------+-------------------+---------------+---------------+--------------+----+-----------+--------------------+--------------------+--------------------+------------+----------------+-------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: integer (nullable = true)\n",
      " |-- county_code: integer (nullable = true)\n",
      " |-- site_num: integer (nullable = true)\n",
      " |-- parameter_code: integer (nullable = true)\n",
      " |-- poc: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- datum: string (nullable = true)\n",
      " |-- parameter_name: string (nullable = true)\n",
      " |-- sample_duration: string (nullable = true)\n",
      " |-- pollutant_standard: string (nullable = true)\n",
      " |-- date_local: timestamp (nullable = true)\n",
      " |-- units_of_measure: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- observation_count: integer (nullable = true)\n",
      " |-- observation_percent: double (nullable = true)\n",
      " |-- arithmetic_mean: double (nullable = true)\n",
      " |-- first_max_value: double (nullable = true)\n",
      " |-- first_max_hour: integer (nullable = true)\n",
      " |-- aqi: string (nullable = true)\n",
      " |-- method_code: integer (nullable = true)\n",
      " |-- method_name: string (nullable = true)\n",
      " |-- local_site_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- cbsa_name: string (nullable = true)\n",
      " |-- date_of_last_change: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"pollution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = spark.sql (\"SELECT COUNT(1)FROM pollution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 8097069|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT DISTINCT parameter_name FROM pollution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      parameter_name|\n",
      "+--------------------+\n",
      "|             Benzene|\n",
      "|Chromium VI (TSP) LC|\n",
      "|  12-Dichloropropane|\n",
      "|Acrolein - Unveri...|\n",
      "| Tetrachloroethylene|\n",
      "|       Lead PM10 STP|\n",
      "|     Nickel PM10 STP|\n",
      "|    Nickel (TSP) STP|\n",
      "|   Chromium PM10 STP|\n",
      "|  Ethylene dibromide|\n",
      "|    Mercury PM10 STP|\n",
      "|  Chromium (TSP) STP|\n",
      "|cis-13-Dichloropr...|\n",
      "|      Vinyl chloride|\n",
      "|   Trichloroethylene|\n",
      "|       Lead PM2.5 LC|\n",
      "|          Chloroform|\n",
      "|  Manganese PM10 STP|\n",
      "|        Acetaldehyde|\n",
      "|  Beryllium PM2.5 LC|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari rata rata konsentrasi air polutant terbesar dalam suatu state (ex: pennsylvania)\n",
    "\n",
    "query1 = spark.sql (\"SELECT parameter_name, AVG(arithmetic_mean),units_of_measure FROM pollution\\\n",
    "                        WHERE state_name = 'Pennsylvania' \\\n",
    "                        GROUP BY parameter_name, units_of_measure \\\n",
    "                        ORDER BY AVG(arithmetic_mean) DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|      parameter_name|avg(arithmetic_mean)|    units_of_measure|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|        Formaldehyde|  2.6133003936669663|Parts per billion...|\n",
      "|             Benzene|  2.2339411705663816|Parts per billion...|\n",
      "|        Acetaldehyde|  2.0236999796918775|Parts per billion...|\n",
      "|   Trichloroethylene| 0.24696496444731733|Parts per billion...|\n",
      "|     Dichloromethane|  0.2274077303487766|Parts per billion...|\n",
      "|Acrolein - Unveri...| 0.22246105104761915|Parts per billion...|\n",
      "| Tetrachloroethylene| 0.20841215255332896|Parts per billion...|\n",
      "|        13-Butadiene| 0.20047134502923972|Parts per billion...|\n",
      "|Carbon tetrachloride| 0.09884115055713964|Parts per billion...|\n",
      "|          Chloroform| 0.03560626050963646|Parts per billion...|\n",
      "| Beryllium (TSP) STP| 0.02544813278008298|Nanograms/cubic m...|\n",
      "| Manganese (TSP) STP|0.025297648530331437|Micrograms/cubic ...|\n",
      "| Ethylene dichloride|0.023378231644260547|Parts per billion...|\n",
      "|      Vinyl chloride| 0.00679436109674082|Parts per billion...|\n",
      "|  12-Dichloropropane|0.006779113351428201|Parts per billion...|\n",
      "|  Chromium (TSP) STP| 0.00611318138651472|Micrograms/cubic ...|\n",
      "|    Nickel (TSP) STP|0.005194829527682205|Micrograms/cubic ...|\n",
      "|       Lead PM2.5 LC|0.004860791895866215|Micrograms/cubic ...|\n",
      "|  Manganese PM2.5 LC|0.002959223898480...|Micrograms/cubic ...|\n",
      "|  Ethylene dibromide|0.002640238372846...|Parts per billion...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state yang memiliki rata rata suatu air polutant (ex: Benzene) paling banyak\n",
    "\n",
    "query2 = spark.sql (\"SELECT state_name, AVG(arithmetic_mean), units_of_measure FROM pollution\\\n",
    "                        WHERE parameter_name = 'Benzene'\\\n",
    "                        GROUP BY state_name, units_of_measure\\\n",
    "                        ORDER BY AVG(arithmetic_mean) DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+\n",
      "|       state_name|avg(arithmetic_mean)|    units_of_measure|\n",
      "+-----------------+--------------------+--------------------+\n",
      "|Country Of Mexico|   8.708797468354431|Parts per billion...|\n",
      "|          Alabama|   6.430967802146523|Parts per billion...|\n",
      "|           Kansas|   5.406358490566038|Parts per billion...|\n",
      "|           Alaska|   4.649691056910569|Parts per billion...|\n",
      "|       New Mexico|  4.2419974639498434|Parts per billion...|\n",
      "|         Missouri|  4.0470664072806475|Parts per billion...|\n",
      "|         Colorado|   3.530716491661107|Parts per billion...|\n",
      "|          Arizona|  3.2813660599781884|Parts per billion...|\n",
      "|            Texas|   3.165615802749503|Parts per billion...|\n",
      "|         Arkansas|  3.1010606060606065|Parts per billion...|\n",
      "|             Utah|   3.088827945776851|Parts per billion...|\n",
      "|      Puerto Rico|   3.016304182509505|Parts per billion...|\n",
      "|       California|  2.9796335305933703|Parts per billion...|\n",
      "|             Ohio|    2.78797701521528|Parts per billion...|\n",
      "|         Maryland|  2.5123662364128325|Parts per billion...|\n",
      "|         New York|  2.5023879841191494|Parts per billion...|\n",
      "|            Idaho|  2.4039002624671917|Parts per billion...|\n",
      "|          Florida|  2.4032083447717723|Parts per billion...|\n",
      "|         Michigan|   2.370403475363904|Parts per billion...|\n",
      "|        Louisiana|  2.3645278055383474|Parts per billion...|\n",
      "+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nilai rata-rata konsentrasi zat polutan dari tahun ke tahun pada US\n",
    "\n",
    "query3 = spark.sql (\"SELECT EXTRACT (YEAR FROM date_local), parameter_name, AVG(arithmetic_mean), units_of_measure FROM pollution\\\n",
    "                    GROUP BY EXTRACT(YEAR FROM date_local), units_of_measure, parameter_name \\\n",
    "                    ORDER BY EXTRACT(YEAR FROM date_local) DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------------+--------------------+--------------------+\n",
      "|year(CAST(date_local AS DATE))|      parameter_name|avg(arithmetic_mean)|    units_of_measure|\n",
      "+------------------------------+--------------------+--------------------+--------------------+\n",
      "|                          2017|   Trichloroethylene|0.003557620817843...|Parts per billion...|\n",
      "|                          2017|  Chromium (TSP) STP|0.006750087719298247|Micrograms/cubic ...|\n",
      "|                          2017|        13-Butadiene| 0.10028373702422147|Parts per billion...|\n",
      "|                          2017|      Vinyl chloride|                 0.0|Parts per billion...|\n",
      "|                          2017|     Dichloromethane|  0.6513717472118958|Parts per billion...|\n",
      "|                          2017|             Benzene|  1.6149861979010498|Parts per billion...|\n",
      "|                          2017|1122-Tetrachloroe...|            3.125E-4|Parts per billion...|\n",
      "|                          2017|trans-13-Dichloro...| 0.02972010178117048|Parts per billion...|\n",
      "|                          2017|  Manganese PM10 STP|  16.464444444444442|Nanograms/cubic m...|\n",
      "|                          2017|          Chloroform|0.014985130111524166|Parts per billion...|\n",
      "|                          2017|  Beryllium PM10 STP|                 0.0|Nanograms/cubic m...|\n",
      "|                          2017| Tetrachloroethylene|0.020527881040892197|Parts per billion...|\n",
      "|                          2017|  Ethylene dibromide|0.001505376344086...|Parts per billion...|\n",
      "|                          2017|Carbon tetrachloride| 0.08852985074626868|Parts per billion...|\n",
      "|                          2017| Ethylene dichloride|0.004580645161290...|Parts per billion...|\n",
      "|                          2017|  12-Dichloropropane|           7.8125E-4|Parts per billion...|\n",
      "|                          2017|    Arsenic PM10 STP|               1.072|Nanograms/cubic m...|\n",
      "|                          2017|   Chromium PM10 STP|               23.08|Nanograms/cubic m...|\n",
      "|                          2017|Acrolein - Unveri...|  0.8333516483516484|Parts per billion...|\n",
      "|                          2017| Acrolein - Verified|   1.674871794871795|Parts per billion...|\n",
      "+------------------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nilai konsentrasi benzene dari tahun ke tahun pada US\n",
    "\n",
    "query4 = spark.sql (\"SELECT EXTRACT (YEAR FROM date_local), AVG(arithmetic_mean), units_of_measure FROM pollution\\\n",
    "                    WHERE parameter_name = 'Benzene' \\\n",
    "                    GROUP BY EXTRACT(YEAR FROM date_local), units_of_measure \\\n",
    "                    ORDER BY EXTRACT(YEAR FROM date_local) DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------------+--------------------+\n",
      "|year(CAST(date_local AS DATE))|avg(arithmetic_mean)|    units_of_measure|\n",
      "+------------------------------+--------------------+--------------------+\n",
      "|                          2017|  1.6149861979010498|Parts per billion...|\n",
      "|                          2016|  1.2660918363206974|Parts per billion...|\n",
      "|                          2015|  1.3288374331274566|Parts per billion...|\n",
      "|                          2014|   1.324096420975431|Parts per billion...|\n",
      "|                          2013|   1.451811187132259|Parts per billion...|\n",
      "|                          2012|   1.603766487430922|Parts per billion...|\n",
      "|                          2011|  1.5579733075711695|Parts per billion...|\n",
      "|                          2010|  1.7227266005456587|Parts per billion...|\n",
      "|                          2009|   1.894231711105646|Parts per billion...|\n",
      "|                          2008|  2.3998395963532064|Parts per billion...|\n",
      "|                          2007|  2.1549146476617076|Parts per billion...|\n",
      "|                          2006|  2.1611982857310266|Parts per billion...|\n",
      "|                          2005|   2.524185586363823|Parts per billion...|\n",
      "|                          2004|  2.3579587869236303|Parts per billion...|\n",
      "|                          2003|  2.6067226590305443|Parts per billion...|\n",
      "|                          2002|   2.591102267914845|Parts per billion...|\n",
      "|                          2001|  3.0948410008970404|Parts per billion...|\n",
      "|                          2000|   3.103127915706775|Parts per billion...|\n",
      "|                          1999|  2.9931887320577117|Parts per billion...|\n",
      "|                          1998|  3.0668340231472353|Parts per billion...|\n",
      "+------------------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1753.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange rangepartitioning(year(CAST(date_local AS DATE))#930 DESC NULLS LAST, 200)\n+- *(2) HashAggregate(keys=[year(cast(date_local#21 as date))#954, units_of_measure#22], functions=[avg(arithmetic_mean#26)], output=[year(CAST(date_local AS DATE))#930, avg(arithmetic_mean)#929, units_of_measure#22])\n   +- Exchange hashpartitioning(year(cast(date_local#21 as date))#954, units_of_measure#22, 200)\n      +- *(1) HashAggregate(keys=[year(cast(date_local#21 as date)) AS year(cast(date_local#21 as date))#954, units_of_measure#22], functions=[partial_avg(arithmetic_mean#26)], output=[year(cast(date_local#21 as date))#954, units_of_measure#22, sum#947, count#948L])\n         +- *(1) Project [date_local#21, units_of_measure#22, arithmetic_mean#26]\n            +- *(1) Filter (isnotnull(parameter_name#18) && (parameter_name#18 = Benzene))\n               +- *(1) FileScan csv [parameter_name#18,date_local#21,units_of_measure#22,arithmetic_mean#26] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/gunstringer/Downloads/epa_hap_daily_summary.csv], PartitionFilters: [], PushedFilters: [IsNotNull(parameter_name), EqualTo(parameter_name,Benzene)], ReadSchema: struct<parameter_name:string,date_local:timestamp,units_of_measure:string,arithmetic_mean:double>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-5bafb7b89e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spark/less_2_hour_rides\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1753.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange rangepartitioning(year(CAST(date_local AS DATE))#930 DESC NULLS LAST, 200)\n+- *(2) HashAggregate(keys=[year(cast(date_local#21 as date))#954, units_of_measure#22], functions=[avg(arithmetic_mean#26)], output=[year(CAST(date_local AS DATE))#930, avg(arithmetic_mean)#929, units_of_measure#22])\n   +- Exchange hashpartitioning(year(cast(date_local#21 as date))#954, units_of_measure#22, 200)\n      +- *(1) HashAggregate(keys=[year(cast(date_local#21 as date)) AS year(cast(date_local#21 as date))#954, units_of_measure#22], functions=[partial_avg(arithmetic_mean#26)], output=[year(cast(date_local#21 as date))#954, units_of_measure#22, sum#947, count#948L])\n         +- *(1) Project [date_local#21, units_of_measure#22, arithmetic_mean#26]\n            +- *(1) Filter (isnotnull(parameter_name#18) && (parameter_name#18 = Benzene))\n               +- *(1) FileScan csv [parameter_name#18,date_local#21,units_of_measure#22,arithmetic_mean#26] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/gunstringer/Downloads/epa_hap_daily_summary.csv], PartitionFilters: [], PushedFilters: [IsNotNull(parameter_name), EqualTo(parameter_name,Benzene)], ReadSchema: struct<parameter_name:string,date_local:timestamp,units_of_measure:string,arithmetic_mean:double>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "data = [go.Histogram(x=query4.toPandas()['d1'])]\n",
    "py.iplot(data, filename=\"spark/less_2_hour_rides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
